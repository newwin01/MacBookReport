{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# #Mount your drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"FGrw6kiveUAx","executionInfo":{"status":"ok","timestamp":1699012972878,"user_tz":-540,"elapsed":11,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Code\n","1.   Implement and apply kNN on MNIST with k=1, 5, 10. Apply kNN on raw images, and 2, 7 dimensional eigenspaces, respectively. Show the accuracy scores for each run (you'd run the algorithm 9 times).\n","\n","2.   Use and run the Random Forest algorithm for MNIST classification (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Show the accuracy scores.\n","\n","If it takes too much time, reduce the number of samples for training/testing (based on random selection)."],"metadata":{"id":"XtZTSiDRAp2T"}},{"cell_type":"code","source":["#Code here\n","# Code here\n","import six.moves.cPickle as pickle\n","import gzip\n","import os\n","import numpy as np\n","from PIL import Image\n","from numpy import linalg as LA\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from numpy.random import uniform\n","from sklearn.decomposition import PCA"],"metadata":{"id":"sgx7RK2z5g2z","executionInfo":{"status":"ok","timestamp":1699012974247,"user_tz":-540,"elapsed":1379,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def load_data(dataset):\n","    ''' Loads the dataset\n","\n","    :type dataset: string\n","    :param dataset: the path to the dataset (here MNIST)\n","\n","    copied from http://deeplearning.net/ and revised by hchoi\n","    '''\n","\n","    # Download the MNIST dataset if it is not present\n","    if not os.path.isfile(dataset):\n","        origin = (\n","            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n","        )\n","        print('Downloading data from %s' % origin)\n","        urllib.request.urlretrieve(origin, dataset)\n","\n","    print('Loading data...')\n","\n","    # Load the dataset\n","    with gzip.open(dataset, 'rb') as f:\n","        try:\n","            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n","        except:\n","            train_set, valid_set, test_set = pickle.load(f)\n","    # train_set, valid_set, test_set format: tuple(input, target)\n","    # input is a numpy.ndarray of 2 dimensions (a matrix)\n","    # where each row corresponds to an example. target is a\n","    # numpy.ndarray of 1 dimension (vector) that has the same length as\n","    # the number of rows in the input. It should give the target\n","    # to the example with the same index in the input.\n","    print('... data has been loaded!')\n","    return train_set, valid_set, test_set"],"metadata":{"id":"bSbaCZQT5h-e","executionInfo":{"status":"ok","timestamp":1699012974247,"user_tz":-540,"elapsed":10,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Load the data into train, validation and test sets\n","train_set, val_set, test_set = load_data('mnist.pkl.gz')\n","\n","#Separate each set into image vector (_x) and label (_y)\n","train_x, train_y = train_set\n","val_x, val_y = val_set\n","test_x, test_y = test_set\n","\n","desired_size = 5000\n","\n","#Due to Computational Expensiveness\n","\n","np.random.seed(0)\n","indices = np.random.choice(train_x.shape[0], desired_size, replace=False)\n","train_x = train_x[indices]\n","train_y = train_y[indices]\n","\n","print(train_x.shape)\n","print(train_y.shape)\n","\n","desired_size = 5000\n","\n","np.random.seed(0)\n","indices = np.random.choice(test_x.shape[0], desired_size, replace=False)\n","test_x = test_x[indices]\n","test_y = test_y[indices]\n","\n","print(test_x.shape)\n","print(test_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4gfWtCM5i_j","executionInfo":{"status":"ok","timestamp":1699012976072,"user_tz":-540,"elapsed":1833,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}},"outputId":"6017b4c2-6408-42d4-d7bf-aeb5c65d0671"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","... data has been loaded!\n","(5000, 784)\n","(5000,)\n","(5000, 784)\n","(5000,)\n"]}]},{"cell_type":"code","source":["def predictNeighbors(train_x_set, train_y_set, test_x, k):\n","  dists = []\n","  for i, train_row in enumerate(train_x_set):\n","      dist = np.linalg.norm(train_row - test_x)\n","      dists.append((train_y_set[i], dist))\n","  dists.sort(key=lambda x: x[1])\n","\n","  neighbors = dists[:k]\n","\n","  class_counts = {}\n","  for neighbor in neighbors:\n","      label, _ = neighbor\n","      if label in class_counts:\n","          class_counts[label] += 1\n","      else:\n","          class_counts[label] = 1\n","\n","  predicted_class = max(class_counts, key=class_counts.get)\n","\n","  return predicted_class"],"metadata":{"id":"NCD4dFE_6wUa","executionInfo":{"status":"ok","timestamp":1699012976072,"user_tz":-540,"elapsed":6,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def calculate_accuracy(train_x_set, train_y, test_x_set, test_y, k):\n","\n","    correct_predictions = 0\n","    total_predictions = len(test_x_set)\n","\n","    for i, test_x in enumerate(test_x_set):\n","\n","        predicted_class = predictNeighbors(train_x_set, train_y, test_x, k)\n","        if predicted_class == test_y[i]:\n","            correct_predictions += 1\n","\n","    accuracy = correct_predictions / total_predictions\n","    return accuracy"],"metadata":{"id":"c0ppQxmoTLNS","executionInfo":{"status":"ok","timestamp":1699012976073,"user_tz":-540,"elapsed":6,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["accuracy = calculate_accuracy(train_x, train_y, test_x, test_y, k=1)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","accuracy = calculate_accuracy(train_x, train_y, test_x, test_y, k=5)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","accuracy = calculate_accuracy(train_x, train_y, test_x, test_y, k=10)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LkfWpdF6KhGW","executionInfo":{"status":"ok","timestamp":1699017282315,"user_tz":-540,"elapsed":799766,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}},"outputId":"64e3dfc1-4b2c-4fb1-e580-f5282049db1a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 94.08%\n","Accuracy: 94.40%\n","Accuracy: 93.92%\n"]}]},{"cell_type":"code","source":["#code here\n","pca = PCA(n_components=2)\n","PCA_2_train_x = pca.fit_transform(train_x)\n","PCA_2_test_x = pca.transform(test_x)\n","pca = PCA(n_components=7)\n","PCA_7_train_x = pca.fit_transform(train_x)\n","PCA_7_test_x = pca.transform(test_x)"],"metadata":{"id":"J5cik59AByis","executionInfo":{"status":"ok","timestamp":1699013473821,"user_tz":-540,"elapsed":493,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["accuracy = calculate_accuracy(PCA_2_train_x, train_y, PCA_2_test_x, test_y, k=1)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","accuracy = calculate_accuracy(PCA_2_train_x, train_y, PCA_2_test_x, test_y, k=5)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","accuracy = calculate_accuracy(PCA_2_train_x, train_y, PCA_2_test_x, test_y, k=10)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogP4HpdGtqrJ","executionInfo":{"status":"ok","timestamp":1699013908670,"user_tz":-540,"elapsed":434851,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}},"outputId":"5bb52342-104e-4bd5-e5de-8ce95e081fa6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 37.90%\n","Accuracy: 42.14%\n","Accuracy: 44.16%\n"]}]},{"cell_type":"code","source":["accuracy = calculate_accuracy(PCA_7_train_x, train_y, PCA_7_test_x, test_y, k=1)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","accuracy = calculate_accuracy(PCA_7_train_x, train_y, PCA_7_test_x, test_y, k=5)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","accuracy = calculate_accuracy(PCA_7_train_x, train_y, PCA_7_test_x, test_y, k=10)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCtEpCgptu08","executionInfo":{"status":"ok","timestamp":1699014337206,"user_tz":-540,"elapsed":428543,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}},"outputId":"36142141-09ed-4195-f2c4-ad26ce864779"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 81.22%\n","Accuracy: 84.96%\n","Accuracy: 86.06%\n"]}]},{"cell_type":"markdown","source":["Random Forrest"],"metadata":{"id":"MCmJerxzxU7D"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","\n","rf=RandomForestClassifier(n_estimators=100)\n","rf.fit(train_x,train_y)\n","\n","y_predict = rf.predict(test_x)\n","print(f\"Accuracy is {accuracy_score(y_predict, test_y)* 100} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCV2DFxJxXRk","executionInfo":{"status":"ok","timestamp":1699014340321,"user_tz":-540,"elapsed":3130,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}},"outputId":"c1c2c234-10bf-4aec-a80e-9983eccc7bbd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy is 94.08 %\n"]}]},{"cell_type":"markdown","source":["# ChatGPT prompts\n","\n","Now solve the same task with ChatGPT with the following prompts:\n","\n","\n","1.   Can you implement and apply kNN on MNIST (from scratch without using any specialized library) with k=1, 5, 10? Can you show the accuracies of the predictions? Ignore the computational cost of this code.\n","2.   Can you use the same provided knn algorithm and apply it to the MNIST data with reduced dimensions to 2 and 7, with k=1, 5, 10? Please provide the accuracies.\n","3.   Can you provide a code for the random forest algorithm applied to the MNIST dataset? Please use specialized libraries like sklearn. Please show the accuracies scores.\n","\n","\n","**Paste the answer provided and run the code below:**"],"metadata":{"id":"6idIZhGPB1wZ"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","\n","# Function to calculate Euclidean distance between two images\n","def euclidean_distance(image1, image2):\n","    return np.sqrt(np.sum((image1 - image2) ** 2))\n","\n","# Function to predict the label of a test image using k-NN\n","def predict_knn(train_data, train_labels, test_image, k):\n","    distances = []\n","    for i in range(len(train_data)):\n","        dist = euclidean_distance(train_data[i], test_image)\n","        distances.append((train_labels[i], dist))\n","\n","    distances.sort(key=lambda x: x[1])\n","    k_nearest = [label for label, _ in distances[:k]]\n","    most_common = Counter(k_nearest).most_common(1)\n","    return most_common[0][0]\n","\n","# Function to calculate accuracy\n","def calculate_accuracy(train_data, train_labels, test_data, test_labels, k):\n","    correct = 0\n","    for i in range(len(test_data)):\n","        prediction = predict_knn(train_data, train_labels, test_data[i], k)\n","        if prediction == test_labels[i]:\n","            correct += 1\n","    accuracy = correct / len(test_data)\n","    return accuracy\n","\n","# Load MNIST dataset (you need to have the data in the appropriate format)\n","# Replace 'train_data', 'train_labels', 'test_data', and 'test_labels' with your data\n","# Each image in 'train_data' and 'test_data' should be a flattened array of pixel values, and labels should be integers.\n","# Ensure that train_data and test_data have the same number of features (784 for MNIST).\n","\n","\n","# Comments: ChatGPT kept failed to load MNIST dataset, So I provided context\n","train_data = train_x\n","train_labels = train_y\n","test_data = test_x\n","test_labels = test_y\n","\n","# Set k values\n","k_values = [1, 5, 10]\n","\n","# Calculate and print accuracies for each k\n","for k in k_values:\n","    accuracy = calculate_accuracy(train_data, train_labels, test_data, test_labels, k)\n","    print(f'Accuracy for k={k}: {accuracy}')\n","\n","# Reduce dimensionality to 2 and 7 using PCA for both training and testing data\n","pca_2 = PCA(n_components=2)\n","reduced_train_data_2 = pca_2.fit_transform(train_data)\n","reduced_test_data_2 = pca_2.transform(test_data)\n","pca_7 = PCA(n_components=7)\n","reduced_train_data_7 = pca_7.fit_transform(train_data)\n","reduced_test_data_7 = pca_7.transform(test_data)\n","\n","# Calculate and print accuracies for dimensionality reduction to 2\n","print(\"Accuracy for Dimensionality Reduction to 2:\")\n","for k in k_values:\n","    accuracy = calculate_accuracy(reduced_train_data_2, train_labels, reduced_test_data_2, test_labels, k)\n","    print(f'Accuracy for k={k}: {accuracy}')\n","\n","# Calculate and print accuracies for dimensionality reduction to 7\n","print(\"\\nAccuracy for Dimensionality Reduction to 7:\")\n","for k in k_values:\n","    accuracy = calculate_accuracy(reduced_train_data_7, train_labels, reduced_test_data_7, test_labels, k)\n","    print(f'Accuracy for k={k}: {accuracy}')\n","\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load the MNIST dataset (you need to have the data in the appropriate format)\n","# Replace 'X' with your data (each row is a flattened image), and 'y' with your labels.\n","# Ensure that X has the same number of features (784 for MNIST).\n","\n","X = train_x  # Replace with your feature data\n","y = train_y  # Replace with your labels\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a Random Forest classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# Fit the classifier to the training data\n","rf_classifier.fit(X_train, y_train)\n","\n","# Make predictions on the testing data\n","y_pred = rf_classifier.predict(X_test)\n","\n","# Calculate accuracy score\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy}')"],"metadata":{"id":"yNdFVuAzBz-6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699016481916,"user_tz":-540,"elapsed":2141602,"user":{"displayName":"장세창학부생","userId":"09948416410105716187"}},"outputId":"000565af-1293-4705-e0d0-b181141f0f52"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for k=1: 0.9408\n","Accuracy for k=5: 0.944\n","Accuracy for k=10: 0.9392\n","Accuracy for Dimensionality Reduction to 2:\n","Accuracy for k=1: 0.3794\n","Accuracy for k=5: 0.4212\n","Accuracy for k=10: 0.4416\n","\n","Accuracy for Dimensionality Reduction to 7:\n","Accuracy for k=1: 0.8122\n","Accuracy for k=5: 0.8494\n","Accuracy for k=10: 0.8604\n","Accuracy: 0.925\n"]}]},{"cell_type":"markdown","source":["# Report\n","\n","\n","1.   What are some differences you notice between your code and ChatGPT's? (Name at least 3)\n","2.   Ask ChatGPT to compare kNN and Random Forest. Paste the answer here. Do you agree with ChatGPT's statement? Why yes or why not?\n","3.   In yours and ChatGPTs implementation, which algorithm had the highest accuracy?\n","4.   Conclude with some thoughts and things you learned from this homework."],"metadata":{"id":"TZPU5Iz9C2IS"}},{"cell_type":"markdown","source":["What are some differences you notice between your code and ChatGPT's? (Name at least 3)\n","1.\tChatGPT defined its own function to calculate Euclidean distance, but I used a built-in function in NumPy which is np.linalg.norm function\n","2.\tTo count the neighbors, I sorted the list by the shortest ones and counted each label not using any libraries. ChatGPT’s code used “from collections import Counter” to count the most common labels in the list.\n","3.\tFor the random forest implementation, ChatGPT used train_x and train_y for the whole data and label set and split into train and test dataset again. However, my code used train and data set as I used. I think it is ChatGPT’s limitation, just providing higher(frequent) tokens.\n","\n","\n","Ask ChatGPT to compare kNN and Random Forest. Paste the answer here. Do you agree with ChatGPT's statement? Why yes or why not?\n","\n","k-Nearest Neighbors (k-NN) and Random Forest are both machine learning algorithms used for classification tasks, but they have different characteristics, advantages, and disadvantages. Let's compare these two algorithms:\n","\n","1. Model Type:\n","\n","    k-NN: k-NN is an instance-based or lazy learning algorithm. It makes predictions based on the majority class among the k-nearest neighbors of a data point.\n","    Random Forest: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions.\n","\n","2. Training:\n","\n","    k-NN: k-NN doesn't involve traditional training. It stores the entire training dataset in memory and uses it for making predictions.\n","    Random Forest: Random Forest trains multiple decision trees on different subsets of the data with bootstrapping (bagging) and feature selection (random feature subspace). Each tree is trained independently.\n","\n","3. Interpretability:\n","\n","    k-NN: It is straightforward to understand and explain since it's based on the similarity of data points.\n","    Random Forest: It's more complex and challenging to interpret because it consists of many decision trees.\n","\n","4. Complexity:\n","\n","    k-NN: Predictions can be slow, especially for large datasets, as it needs to calculate distances to all training points.\n","    Random Forest: Predictions are generally faster due to the ensemble of decision trees. Training can be slower, but it can be parallelized.\n","\n","5. Overfitting:\n","\n","    k-NN: Prone to overfitting, especially with small values of k. The choice of k is crucial.\n","    Random Forest: Less prone to overfitting, thanks to the ensemble nature and the randomness introduced during training.\n","\n","6. Parameter Selection:\n","\n","    k-NN: Main parameter is k (the number of neighbors to consider). You need to choose an appropriate k value.\n","    Random Forest: Parameters include the number of trees, maximum depth of trees, and others. Hyperparameter tuning is required.\n","\n","7. Performance:\n","\n","    k-NN: It may perform well with small to medium-sized datasets, but it can suffer from the \"curse of dimensionality\" as the number of features increases.\n","    Random Forest: Generally performs well on a wide range of datasets, including high-dimensional data.\n","\n","8. Handling Imbalanced Data:\n","\n","    k-NN: Sensitive to imbalanced datasets as it may be biased toward the majority class.\n","    Random Forest: Better at handling imbalanced data, as it considers class balances during tree construction.\n","\n","9. Versatility:\n","\n","    k-NN: Suitable for a wide range of applications but less versatile than Random Forest.\n","    Random Forest: Versatile and applicable to various machine-learning tasks, including classification and regression.\n","\n","10. Scalability:\n","- k-NN: Scales poorly with many data points due to the need for calculating distances to all training samples.\n","- Random Forest: More scalable and can handle large datasets.\n","\n","In summary, the choice between k-NN and Random Forest depends on the specific characteristics of your dataset, the desired interpretability, and the trade-offs between training and prediction time. Random Forest is often a robust choice for many scenarios, while k-NN can be useful for smaller datasets with low dimensionality and simple interpretability requirements.\n","\n","My answer: I agreed with ChatGPT’s answer. ChatGPT answered the difference between kNN and Random Forest in the various perspectives. It is true that k-NN is instance-based and lazy learning, and Random Forest is ensemble learning. Also, k-NN is straightforward Random Forest is more complex. Random Forest is ensemble learning combining multiple decision trees. The decision tree is quite straightforward, but its ensemble learning is more complicated However, I think both algorithms are quite straightforward other than machine learning algorithms. I also agreed with other opinions as well. Most importantly, k-NN takes a long for the large dataset since it requires the calculation of all datasets. Also, Random Forest can be slower, but it might be better to use it on a large dataset since the prediction is faster.\n","\n","In yours and ChatGPTs implementation, which algorithm had the highest accuracy?\n","\n","The highest accuracy algorithm is kNN algorithm with 5 neighbors However, there is no big difference between the random forest. It might be different depending on the dataset and number of datasets.\n","\n","Conclude with some thoughts and things you learned from this homework.\n","\n","In this homework, I could see how kNN works. kNN algorithms are popular algorithms that are used in various situations. Also, we conducted kNN on the MNIST dataset with different dimensions. Using 2-dimensional eigenspaces, the accuracy has been decreased significantly. Only reducing the dimensions is not the best choice all the time. Also, it was the first time to see how ensemble learning works. Using the sets of the model and choose can be the breakthrough.\n"],"metadata":{"id":"TFfifIJLQ6a4"}}]}